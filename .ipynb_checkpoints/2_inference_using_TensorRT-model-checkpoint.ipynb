{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the needed libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.tensorrt as trt # must import this although we will not use it explicitly\n",
    "from tensorflow.python.platform import gfile\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# read the testing images (only for example)\n",
    "img1= Image.open(\"dataset/mnist/testing/0/img_108.jpg\")\n",
    "img2= Image.open(\"dataset/mnist/testing/1/img_0.jpg\")\n",
    "img1 = np.asarray(img1)\n",
    "img2 = np.asarray(img2)\n",
    "input_img = np.concatenate((img1.reshape((1, 28, 28, 1)), \n",
    "                            img2.reshape((1, 28, 28, 1))), \n",
    "                           axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to read \".pb\" model (TensorRT model is stored in \".pb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read a \".pb\" model \n",
    "# (can be used to read frozen model or TensorRT model)\n",
    "def read_pb_graph(model):\n",
    "  with gfile.FastGFile(model,'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "  return graph_def"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform inference using TensorRT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-a1064865cc5f>:4: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.gfile.GFile.\n",
      "needed time in inference-0:  0.0007970333099365234\n",
      "needed time in inference-1:  0.0008862018585205078\n",
      "needed time in inference-2:  0.0005896091461181641\n",
      "needed time in inference-3:  0.0007572174072265625\n",
      "needed time in inference-4:  0.0006136894226074219\n",
      "needed time in inference-5:  0.0008072853088378906\n",
      "needed time in inference-6:  0.0006251335144042969\n",
      "needed time in inference-7:  0.0009069442749023438\n",
      "needed time in inference-8:  0.00069427490234375\n",
      "needed time in inference-9:  0.0005767345428466797\n",
      "needed time in inference-10:  0.0005242824554443359\n",
      "needed time in inference-11:  0.0006775856018066406\n",
      "needed time in inference-12:  0.0006916522979736328\n",
      "needed time in inference-13:  0.0007081031799316406\n",
      "needed time in inference-14:  0.0005643367767333984\n",
      "needed time in inference-15:  0.0006608963012695312\n",
      "needed time in inference-16:  0.00066375732421875\n",
      "needed time in inference-17:  0.0005910396575927734\n",
      "needed time in inference-18:  0.00047969818115234375\n",
      "needed time in inference-19:  0.0005185604095458984\n",
      "needed time in inference-20:  0.0005147457122802734\n",
      "needed time in inference-21:  0.0006000995635986328\n",
      "needed time in inference-22:  0.0006248950958251953\n",
      "needed time in inference-23:  0.0005657672882080078\n",
      "needed time in inference-24:  0.0005025863647460938\n",
      "needed time in inference-25:  0.0006358623504638672\n",
      "needed time in inference-26:  0.0005660057067871094\n",
      "needed time in inference-27:  0.0006372928619384766\n",
      "needed time in inference-28:  0.0008716583251953125\n",
      "needed time in inference-29:  0.0006303787231445312\n",
      "needed time in inference-30:  0.0007030963897705078\n",
      "needed time in inference-31:  0.0005540847778320312\n",
      "needed time in inference-32:  0.0005991458892822266\n",
      "needed time in inference-33:  0.0006985664367675781\n",
      "needed time in inference-34:  0.0007381439208984375\n",
      "needed time in inference-35:  0.0007929801940917969\n",
      "needed time in inference-36:  0.0006864070892333984\n",
      "needed time in inference-37:  0.0007121562957763672\n",
      "needed time in inference-38:  0.0007176399230957031\n",
      "needed time in inference-39:  0.0007545948028564453\n",
      "needed time in inference-40:  0.0006668567657470703\n",
      "needed time in inference-41:  0.0005671977996826172\n",
      "needed time in inference-42:  0.0005640983581542969\n",
      "needed time in inference-43:  0.0006301403045654297\n",
      "needed time in inference-44:  0.000614166259765625\n",
      "needed time in inference-45:  0.0008184909820556641\n",
      "needed time in inference-46:  0.0007128715515136719\n",
      "needed time in inference-47:  0.0007388591766357422\n",
      "needed time in inference-48:  0.0006651878356933594\n",
      "needed time in inference-49:  0.0006155967712402344\n",
      "average inference time:  0.0006606721878051758\n"
     ]
    }
   ],
   "source": [
    "# variable\n",
    "TENSORRT_MODEL_PATH = './model/TensorRT_model.pb'\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.50))) as sess:\n",
    "        # read TensorRT model\n",
    "        trt_graph = read_pb_graph(TENSORRT_MODEL_PATH)\n",
    "\n",
    "        # obtain the corresponding input-output tensor\n",
    "        tf.import_graph_def(trt_graph, name='')\n",
    "        input = sess.graph.get_tensor_by_name('input_tensor_input:0')\n",
    "        output = sess.graph.get_tensor_by_name('output_tensor/Softmax:0')\n",
    "\n",
    "        # in this case, it demonstrates to perform inference for 50 times\n",
    "        total_time = 0; n_time_inference = 50\n",
    "        out_pred = sess.run(output, feed_dict={input: input_img})\n",
    "        for i in range(n_time_inference):\n",
    "            t1 = time.time()\n",
    "            out_pred = sess.run(output, feed_dict={input: input_img})\n",
    "            t2 = time.time()\n",
    "            delta_time = t2 - t1\n",
    "            total_time += delta_time\n",
    "            print(\"needed time in inference-\" + str(i) + \": \", delta_time)\n",
    "        avg_time_tensorRT = total_time / n_time_inference\n",
    "        print(\"average inference time: \", avg_time_tensorRT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform inference using the original tensorflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "needed time in inference-0:  0.0008623600006103516\n",
      "needed time in inference-1:  0.0006482601165771484\n",
      "needed time in inference-2:  0.0005645751953125\n",
      "needed time in inference-3:  0.0006949901580810547\n",
      "needed time in inference-4:  0.0006227493286132812\n",
      "needed time in inference-5:  0.00063323974609375\n",
      "needed time in inference-6:  0.0007140636444091797\n",
      "needed time in inference-7:  0.000591278076171875\n",
      "needed time in inference-8:  0.0006506443023681641\n",
      "needed time in inference-9:  0.0006706714630126953\n",
      "needed time in inference-10:  0.0006504058837890625\n",
      "needed time in inference-11:  0.0006966590881347656\n",
      "needed time in inference-12:  0.0006566047668457031\n",
      "needed time in inference-13:  0.0007524490356445312\n",
      "needed time in inference-14:  0.0005981922149658203\n",
      "needed time in inference-15:  0.0011470317840576172\n",
      "needed time in inference-16:  0.001070261001586914\n",
      "needed time in inference-17:  0.0007946491241455078\n",
      "needed time in inference-18:  0.0007913112640380859\n",
      "needed time in inference-19:  0.0008008480072021484\n",
      "needed time in inference-20:  0.0006232261657714844\n",
      "needed time in inference-21:  0.0007770061492919922\n",
      "needed time in inference-22:  0.0008072853088378906\n",
      "needed time in inference-23:  0.0006368160247802734\n",
      "needed time in inference-24:  0.0007352828979492188\n",
      "needed time in inference-25:  0.0006897449493408203\n",
      "needed time in inference-26:  0.0006015300750732422\n",
      "needed time in inference-27:  0.0006504058837890625\n",
      "needed time in inference-28:  0.000667572021484375\n",
      "needed time in inference-29:  0.0007107257843017578\n",
      "needed time in inference-30:  0.0007431507110595703\n",
      "needed time in inference-31:  0.0006594657897949219\n",
      "needed time in inference-32:  0.0006804466247558594\n",
      "needed time in inference-33:  0.0007386207580566406\n",
      "needed time in inference-34:  0.0009188652038574219\n",
      "needed time in inference-35:  0.0010395050048828125\n",
      "needed time in inference-36:  0.0010497570037841797\n",
      "needed time in inference-37:  0.0007326602935791016\n",
      "needed time in inference-38:  0.0010771751403808594\n",
      "needed time in inference-39:  0.0009882450103759766\n",
      "needed time in inference-40:  0.0009593963623046875\n",
      "needed time in inference-41:  0.0009052753448486328\n",
      "needed time in inference-42:  0.0008890628814697266\n",
      "needed time in inference-43:  0.0009000301361083984\n",
      "needed time in inference-44:  0.0008306503295898438\n",
      "needed time in inference-45:  0.0009324550628662109\n",
      "needed time in inference-46:  0.0009562969207763672\n",
      "needed time in inference-47:  0.0009608268737792969\n",
      "needed time in inference-48:  0.0009508132934570312\n",
      "needed time in inference-49:  0.000888824462890625\n",
      "average inference time:  0.0007862472534179688\n"
     ]
    }
   ],
   "source": [
    "# variable\n",
    "FROZEN_MODEL_PATH = './model/frozen_model.pb'\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "        # read TensorRT model\n",
    "        frozen_graph = read_pb_graph(FROZEN_MODEL_PATH)\n",
    "\n",
    "        # obtain the corresponding input-output tensor\n",
    "        tf.import_graph_def(frozen_graph, name='')\n",
    "        input = sess.graph.get_tensor_by_name('input_tensor_input:0')\n",
    "        output = sess.graph.get_tensor_by_name('output_tensor/Softmax:0')\n",
    "\n",
    "        # in this case, it demonstrates to perform inference for 50 times\n",
    "        total_time = 0; n_time_inference = 50\n",
    "        out_pred = sess.run(output, feed_dict={input: input_img})\n",
    "        for i in range(n_time_inference):\n",
    "            t1 = time.time()\n",
    "            out_pred = sess.run(output, feed_dict={input: input_img})\n",
    "            t2 = time.time()\n",
    "            delta_time = t2 - t1\n",
    "            total_time += delta_time\n",
    "            print(\"needed time in inference-\" + str(i) + \": \", delta_time)\n",
    "        avg_time_original_model = total_time / n_time_inference\n",
    "        print(\"average inference time: \", avg_time_original_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the prediction result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the prediction output\n",
    "plt.figure('img 1')\n",
    "plt.imshow(img1, cmap='gray')\n",
    "plt.title('pred:' + str(np.argmax(out_pred[0])), fontsize=22)\n",
    "\n",
    "plt.figure('img 2')\n",
    "plt.imshow(img2, cmap='gray')\n",
    "plt.title('pred:' + str(np.argmax(out_pred[1])), fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Op type not registered 'TRTEngineOp' in binary running on cvrc-H97-D3H. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-cf3f5884c37a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# obtain the corresponding input-output tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrt_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'input_tensor_input:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output_tensor/Softmax:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF1120_GPU/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m                 instructions)\n\u001b[0;32m--> 488\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[1;32m    490\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m~/.conda/envs/TF1120_GPU/lib/python3.5/site-packages/tensorflow/python/framework/importer.py\u001b[0m in \u001b[0;36mimport_graph_def\u001b[0;34m(graph_def, input_map, return_elements, name, op_dict, producer_op_list)\u001b[0m\n\u001b[1;32m    416\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         results = c_api.TF_GraphImportGraphDefWithResults(\n\u001b[0;32m--> 418\u001b[0;31m             graph._c_graph, serialized, options)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    419\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScopedTFImportGraphDefResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Op type not registered 'TRTEngineOp' in binary running on cvrc-H97-D3H. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed."
     ]
    }
   ],
   "source": [
    "# import the needed libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.platform import gfile\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# variable\n",
    "TENSORRT_MODEL_PATH = './model/TensorRT_model.pb'\n",
    "\n",
    "# read the testing images (only for example)\n",
    "img1= Image.open(\"dataset/mnist/testing/0/img_108.jpg\")\n",
    "img2= Image.open(\"dataset/mnist/testing/1/img_0.jpg\")\n",
    "img1 = np.asarray(img1)\n",
    "img2 = np.asarray(img2)\n",
    "input_img = np.concatenate((img1.reshape((1, 28, 28, 1)), img2.reshape((1, 28, 28, 1))), axis=0)\n",
    "\n",
    "\n",
    "# function to read a \".pb\" model (can be used to read frozen model or TensorRT model)\n",
    "def read_pb_graph(model):\n",
    "  with gfile.FastGFile(model,'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "  return graph_def\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "        # read TensorRT model\n",
    "        trt_graph = read_pb_graph(TENSORRT_MODEL_PATH)\n",
    "\n",
    "        # obtain the corresponding input-output tensor\n",
    "        tf.import_graph_def(trt_graph, name='')\n",
    "        input = sess.graph.get_tensor_by_name('input_tensor_input:0')\n",
    "        output = sess.graph.get_tensor_by_name('output_tensor/Softmax:0')\n",
    "\n",
    "        # in this case, it demonstrates to perform inference for 50 times\n",
    "        total_time = 0; n_time_inference = 50\n",
    "        for i in range(n_time_inference):\n",
    "            t1 = time.time()\n",
    "            out_pred = sess.run(output, feed_dict={input: input_img})\n",
    "            t2 = time.time()\n",
    "            delta_time = t2 - t1\n",
    "            total_time += delta_time\n",
    "            print(\"needed time in inference-\" + str(i) + \": \", delta_time)\n",
    "        print(\"average inference time: \", total_time / n_time_inference)\n",
    "\n",
    "        # plot the prediction output\n",
    "        plt.figure('img 1')\n",
    "        plt.imshow(img1, cmap='gray')\n",
    "        plt.title('pred:' + str(np.argmax(out_pred[0])), fontsize=22)\n",
    "\n",
    "        plt.figure('img 2')\n",
    "        plt.imshow(img2, cmap='gray')\n",
    "        plt.title('pred:' + str(np.argmax(out_pred[1])), fontsize=22)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TF1120_GPU]",
   "language": "python",
   "name": "conda-env-TF1120_GPU-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
